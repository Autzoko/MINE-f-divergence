{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = [(np.zeros((3,2)),np.ones(5)),(np.zeros((3,2)),np.ones(5)),(np.ones((3,2)),np.ones(5)),(np.ones((3,2)),np.ones(5)),(np.zeros((3,2)),np.ones(5))]\n",
    "x = [x[0].reshape((-1)) for x in a]\n",
    "np.random.permutation(x)\n",
    "from Gibbs import generate_sw_tuples_batch\n",
    "batch = generate_sw_tuples_batch(10, p=20)\n",
    "S = np.array([x[0][0] for x in batch])\n",
    "w = np.array([x[1] for x in batch])\n",
    "from scipy.optimize import ridder\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from MINE import MINE\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import os\n",
    "def seed_torch(seed=1029):\n",
    "\trandom.seed(seed)\n",
    "\tos.environ['PYTHONHASHSEED'] = str(seed) # 为了禁止hash随机化，使得实验可复现\n",
    "\tnp.random.seed(seed)\n",
    "\ttorch.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "\ttorch.backends.cudnn.benchmark = False\n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "seed_torch()\n",
    "\n",
    "model = MINE(11, 20)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "plot_loss = []\n",
    "w_shuffle = np.random.permutation(w)\n",
    "\n",
    "S_sample = Variable(torch.from_numpy(S).type(torch.FloatTensor), requires_grad=False)#训练数据都是计算图里的叶节点，不需要梯度\n",
    "w_sample = Variable(torch.from_numpy(w).type(torch.FloatTensor), requires_grad=False)\n",
    "w_shuffle_sample = Variable(torch.from_numpy(w_shuffle).type(torch.FloatTensor), requires_grad=False)\n",
    "\n",
    "#自定义数值求解函数 done\n",
    "def solve_f_star(y):\n",
    "    if y.requires_grad:\n",
    "        y_ng = y.detach() #去掉梯度信息，成为普通tensor（这一步可以删掉）\n",
    "    x = []\n",
    "    for y_ in y_ng[:,0]:\n",
    "        y_ = y_.item()\n",
    "        def f_star(x):\n",
    "            return np.log(x)-1/x+1-y_\n",
    "        res = ridder(f_star,0,1 if y_<0 else np.exp(y_),xtol=1e-10)\n",
    "        x.append(res)\n",
    "    return torch.tensor(x, requires_grad=False)\n",
    "\n",
    "pred_xy = model(S_sample, w_sample)\n",
    "pred_x_y = model(S_sample, w_shuffle_sample)\n",
    "\n",
    "loss1 = - torch.mean(pred_xy)\n",
    "t1 = solve_f_star(pred_x_y).reshape((-1,1))\n",
    "loss2 = torch.mean(t1*pred_x_y)\n",
    "loss2_actual = torch.mean(t1-1+torch.log(t1))\n",
    "'''\n",
    "plot_loss.append(loss1.data.numpy())\n",
    "print(loss1.item())\n",
    "'''\n",
    "loss = loss1 + loss2\n",
    "estimation = (-loss1 - loss2_actual).data.numpy()\n",
    "model.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
